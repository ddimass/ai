{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\a.lunev\\Anaconda3\\envs\\ML_GPU\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras import layers, regularizers, optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read from CSV\n",
    "my_data = np.genfromtxt('e:\\EURUSD-2000-2019.csv', delimiter=',', skip_header=1)\n",
    "#my_data = np.concatenate((bin_days, my_data), axis=1)\n",
    "my_data = np.delete(my_data, 1, 1)\n",
    "#for i, data in enumerate(my_data):\n",
    "#    my_data[i][0] = (datetime.strptime(str(data[0]), \"%Y%m%d.0\")).isoweekday()\n",
    "#my_data = my_data[np.logical_not(my_data[:,0] == 7)]\n",
    "#bin_days = to_categorical(my_data[:,0])\n",
    "my_data = np.delete(my_data, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[:,0] = my_data[:,1] - my_data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.delete(my_data, 1, 1)\n",
    "my_data = np.delete(my_data, 1, 1)\n",
    "my_data = np.delete(my_data, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12292])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = np.delete(my_data, 0, 1)\n",
    "my_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read from DB\n",
    "# auth_url = 'http://185.49.144.175/api/token/'\n",
    "# bar_url = 'http://185.49.144.175/api/bars/?timeframe=1'\n",
    "# headers = {'Content-type': 'application/json', 'Content-Encoding': 'utf-8'}\n",
    "# data = {\"username\": \"lim\",\"password\": \"Serialnomberr3\"}\n",
    "# answer = requests.post(auth_url, data=json.dumps(data), headers=headers)\n",
    "# response = answer.json()\n",
    "# token = response['access']\n",
    "# headers = {'Authorization': 'Bearer ' + token}\n",
    "# answer = requests.get(bar_url, headers=headers)\n",
    "# response = answer.json()\n",
    "# f_data = np.zeros((len(response), 6))\n",
    "# for i, bar in enumerate(response):\n",
    "#     date = datetime.strptime( bar['time'], \"%Y-%m-%dT%H:%M:%SZ\" )\n",
    "#     values = [date.isoweekday(), float(bar['open']), float(bar['close']), float(bar['high']), float(bar['low']), float(bar['tick_volume'])]\n",
    "#     f_data[i, :] = values\n",
    "# f_data = np.flip(f_data,0)\n",
    "# bin_days = to_categorical(f_data[:,0])\n",
    "# f_data = np.delete(f_data, 0, 1)\n",
    "# f_data = np.concatenate((bin_days, f_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize tick_volume\n",
    "#f_data = np.delete(my_data, 4, 1)\n",
    "# f_data = np.delete(f_data, 0, 1)\n",
    "# f_data = np.delete(f_data, 0, 1)\n",
    "# f_data = np.delete(f_data, 0, 1)\n",
    "f_data = my_data\n",
    "# for col in range(0,f_data.shape[-1]):\n",
    "#     min = f_data[:1500000, col].min(axis=0)\n",
    "#     max = f_data[:1500000, col].max(axis=0)\n",
    "#     f_data[:, col] = (f_data[:, col] - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "close = my_data[:, 0]\n",
    "plt.plot(range(len(close)), close)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03435] [1.60305]\n"
     ]
    }
   ],
   "source": [
    "#Нормализация цены закрытия\n",
    "minx = min(my_data)\n",
    "maxx = max(my_data)\n",
    "print(minx, maxx)\n",
    "f_data = my_data\n",
    "f_data = (f_data - minx) / (maxx - minx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12292])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем массивы нужных размеров lookback должен быть >= delay\n",
    "lookback=50\n",
    "delay=10\n",
    "re_data = f_data[:f_data.shape[0]//lookback*lookback,].reshape(-1, lookback)[:-1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_float_y = my_data[:my_data.shape[0]//lookback*lookback].reshape(-1,lookback)[1:,]\n",
    "data_bool_y = (data_float_y[:,0] - data_float_y[:,delay-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122798, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_float_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013110000000000177 0.0001 0.001 0.004\n"
     ]
    }
   ],
   "source": [
    "dx_0 = 0.0001\n",
    "dx_1 = 0.001\n",
    "dx_2 = 0.004\n",
    "print(max(abs(data_bool_y)), dx_0, dx_1, dx_2)\n",
    "data_bool_y_1 = (abs(data_bool_y) <= dx_0).astype('int').reshape(-1,1)\n",
    "data_bool_y_2 = ((abs(data_bool_y) > dx_0) * (abs(data_bool_y) <= dx_1) * (data_bool_y > 0)).astype('int').reshape(-1,1)\n",
    "data_bool_y_3 = ((abs(data_bool_y) > dx_1) * (abs(data_bool_y) <= dx_2) * (data_bool_y > 0)).astype('int').reshape(-1,1)\n",
    "data_bool_y_4 = ((abs(data_bool_y) > dx_2) * (data_bool_y > 0)).astype('int').reshape(-1,1)\n",
    "data_bool_y_2_neg = ((abs(data_bool_y) > dx_0) * (abs(data_bool_y) <= dx_1) * (data_bool_y < 0)).astype('int').reshape(-1,1)\n",
    "data_bool_y_3_neg = ((abs(data_bool_y) > dx_1) * (abs(data_bool_y) <= dx_2) * (data_bool_y < 0)).astype('int').reshape(-1,1)\n",
    "data_bool_y_4_neg = ((abs(data_bool_y) > dx_2) * (data_bool_y < 0)).astype('int').reshape(-1,1)\n",
    "data_cat_y = np.concatenate((data_bool_y_1, data_bool_y_2, data_bool_y_3, data_bool_y_4, data_bool_y_2_neg, data_bool_y_3_neg, data_bool_y_4_neg), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data_train = re_data[:60000,]\n",
    "f_data_val = re_data[2000000:2100000,]\n",
    "f_data_test = re_data[60001:,]\n",
    "f_data_train_y = data_cat_y[:60000,]\n",
    "f_data_val_y = data_cat_y[2000000:2100000,]\n",
    "f_data_test_y = data_cat_y[60001:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122798 [30815 40825  4940    73 41286  4773    86]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(data_cat_y), np.sum(data_cat_y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15315632 0.15303323 0.15278706 0.15264639 0.15282223 0.15276947\n",
      " 0.15310357 0.15294531 0.15291015 0.15283981 0.1529629  0.15283981\n",
      " 0.15276947 0.15245296 0.15201336 0.15264639 0.15280464 0.1529629\n",
      " 0.15291015 0.15268155 0.15275189 0.15275189 0.15289256 0.15269914\n",
      " 0.15273431 0.1530684  0.15287498 0.15280464 0.15248813 0.15275189\n",
      " 0.15301565 0.15320907 0.15308599 0.15327941 0.15342008 0.15336733\n",
      " 0.15329699 0.15336733 0.15331458 0.15329699 0.15347283 0.1534025\n",
      " 0.15310357 0.15327941 0.15280464 0.15278706 0.15273431 0.15236504\n",
      " 0.15240021 0.15315632] [0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f_data_train[9], f_data_train_y[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999971, 2)\n",
      "(99970, 2)\n",
      "(2771, 2)\n"
     ]
    }
   ],
   "source": [
    "lookback = 30\n",
    "step = 1\n",
    "delay = 30\n",
    "batch_size = 1\n",
    "f_data_train = f_data[:999971,]\n",
    "f_data_train_y = my_data[29:1000000,1] > f_data_train[:,1]\n",
    "f_data_train_y = f_data_train_y.astype('int32') \n",
    "f_data_val = f_data[5000001:5099971,]\n",
    "f_data_val_y = my_data[5000030:5100000,1] > f_data_val[:,1]\n",
    "f_data_val_y = f_data_val_y.astype('int32')\n",
    "f_data_test = f_data[-2800:-29,]\n",
    "f_data_test_y = my_data[-2771:,1] > f_data_test[:,1]\n",
    "f_data_test_y = f_data_test_y.astype('int32')\n",
    "print(f_data_train.shape)\n",
    "print(f_data_val.shape)\n",
    "print(f_data_test.shape)\n",
    "train_gen = TimeseriesGenerator(f_data_train, f_data_train_y, length=lookback, sampling_rate=step, batch_size=batch_size)\n",
    "val_gen = TimeseriesGenerator(f_data_val, f_data_val_y, length=lookback, sampling_rate=step, batch_size=1)\n",
    "test_gen = TimeseriesGenerator(f_data_test, f_data_test_y, length=lookback, sampling_rate=step, batch_size=1)\n",
    "train_steps = len(train_gen)\n",
    "val_steps = len(val_gen)\n",
    "test_steps = len(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "for ts in train_gen:\n",
    "    ts_r = ts[0].reshape(batch_size,-1)\n",
    "    train_x.append(ts_r[0])\n",
    "    train_y = ts[1]\n",
    "val_x=[]\n",
    "val_y=[]\n",
    "for ts in val_gen:\n",
    "    ts_r = ts[0].reshape(batch_size,-1)\n",
    "    val_x.append(ts_r[0])\n",
    "    val_y = ts[1]\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "for ts in test_gen:\n",
    "    ts_r = ts[0].reshape(batch_size,-1)\n",
    "    val_x.append(ts_r[0])\n",
    "    val_y = ts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/210\n",
      "42000/42000 [==============================] - 26s 607us/step - loss: 1.9739 - acc: 0.0030 - val_loss: 1.9945 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00178, saving model to best-weights.hdf5\n",
      "Epoch 2/210\n",
      "42000/42000 [==============================] - 21s 502us/step - loss: 1.9733 - acc: 0.0030 - val_loss: 1.9935 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00178\n",
      "Epoch 3/210\n",
      "42000/42000 [==============================] - 22s 519us/step - loss: 1.9727 - acc: 0.0030 - val_loss: 1.9926 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00178\n",
      "Epoch 4/210\n",
      "42000/42000 [==============================] - 22s 516us/step - loss: 1.9722 - acc: 0.0030 - val_loss: 1.9917 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00178\n",
      "Epoch 5/210\n",
      "42000/42000 [==============================] - 22s 519us/step - loss: 1.9716 - acc: 0.0030 - val_loss: 1.9908 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00178\n",
      "Epoch 6/210\n",
      "42000/42000 [==============================] - 22s 527us/step - loss: 1.9711 - acc: 0.0030 - val_loss: 1.9898 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00178\n",
      "Epoch 7/210\n",
      "42000/42000 [==============================] - 21s 511us/step - loss: 1.9706 - acc: 0.0030 - val_loss: 1.9890 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00178\n",
      "Epoch 8/210\n",
      "42000/42000 [==============================] - 21s 510us/step - loss: 1.9700 - acc: 0.0030 - val_loss: 1.9881 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00178\n",
      "Epoch 9/210\n",
      "42000/42000 [==============================] - 22s 531us/step - loss: 1.9695 - acc: 0.0030 - val_loss: 1.9872 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00178\n",
      "Epoch 10/210\n",
      "42000/42000 [==============================] - 22s 515us/step - loss: 1.9690 - acc: 0.0030 - val_loss: 1.9863 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00178\n",
      "Epoch 11/210\n",
      "42000/42000 [==============================] - 22s 518us/step - loss: 1.9684 - acc: 0.0030 - val_loss: 1.9854 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00178\n",
      "Epoch 12/210\n",
      "42000/42000 [==============================] - 22s 535us/step - loss: 1.9679 - acc: 0.0030 - val_loss: 1.9845 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00178\n",
      "Epoch 13/210\n",
      "42000/42000 [==============================] - 22s 515us/step - loss: 1.9674 - acc: 0.0030 - val_loss: 1.9836 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00178\n",
      "Epoch 14/210\n",
      "42000/42000 [==============================] - 22s 526us/step - loss: 1.9668 - acc: 0.0032 - val_loss: 1.9827 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00178\n",
      "Epoch 15/210\n",
      "42000/42000 [==============================] - 22s 527us/step - loss: 1.9663 - acc: 0.0039 - val_loss: 1.9819 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00178\n",
      "Epoch 16/210\n",
      "42000/42000 [==============================] - 22s 516us/step - loss: 1.9658 - acc: 0.0045 - val_loss: 1.9810 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.00178\n",
      "Epoch 17/210\n",
      "42000/42000 [==============================] - 22s 531us/step - loss: 1.9653 - acc: 0.0051 - val_loss: 1.9801 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.00178\n",
      "Epoch 18/210\n",
      "42000/42000 [==============================] - 22s 529us/step - loss: 1.9648 - acc: 0.0070 - val_loss: 1.9793 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.00178\n",
      "Epoch 19/210\n",
      "42000/42000 [==============================] - 22s 528us/step - loss: 1.9643 - acc: 0.0110 - val_loss: 1.9785 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.00178\n",
      "Epoch 20/210\n",
      "42000/42000 [==============================] - 22s 531us/step - loss: 1.9638 - acc: 0.0254 - val_loss: 1.9776 - val_acc: 0.0018\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.00178\n",
      "Epoch 21/210\n",
      "42000/42000 [==============================] - 22s 525us/step - loss: 1.9633 - acc: 0.0647 - val_loss: 1.9768 - val_acc: 0.0076\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.00178 to 0.00761, saving model to best-weights.hdf5\n",
      "Epoch 22/210\n",
      "42000/42000 [==============================] - 22s 534us/step - loss: 1.9628 - acc: 0.0930 - val_loss: 1.9760 - val_acc: 0.0424\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.00761 to 0.04244, saving model to best-weights.hdf5\n",
      "Epoch 23/210\n",
      "42000/42000 [==============================] - 22s 525us/step - loss: 1.9623 - acc: 0.0967 - val_loss: 1.9752 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.04244 to 0.06356, saving model to best-weights.hdf5\n",
      "Epoch 24/210\n",
      "42000/42000 [==============================] - 22s 532us/step - loss: 1.9618 - acc: 0.0967 - val_loss: 1.9744 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.06356 to 0.06361, saving model to best-weights.hdf5\n",
      "Epoch 25/210\n",
      "42000/42000 [==============================] - 23s 540us/step - loss: 1.9613 - acc: 0.0967 - val_loss: 1.9736 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.06361\n",
      "Epoch 26/210\n",
      "42000/42000 [==============================] - 22s 524us/step - loss: 1.9609 - acc: 0.0967 - val_loss: 1.9728 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.06361\n",
      "Epoch 27/210\n",
      "42000/42000 [==============================] - 22s 536us/step - loss: 1.9604 - acc: 0.0967 - val_loss: 1.9720 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.06361\n",
      "Epoch 28/210\n",
      "42000/42000 [==============================] - 22s 531us/step - loss: 1.9599 - acc: 0.0967 - val_loss: 1.9712 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.06361\n",
      "Epoch 29/210\n",
      "42000/42000 [==============================] - 22s 534us/step - loss: 1.9594 - acc: 0.0967 - val_loss: 1.9704 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.06361\n",
      "Epoch 30/210\n",
      "42000/42000 [==============================] - 24s 566us/step - loss: 1.9590 - acc: 0.0967 - val_loss: 1.9696 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.06361\n",
      "Epoch 31/210\n",
      "42000/42000 [==============================] - 24s 562us/step - loss: 1.9585 - acc: 0.0967 - val_loss: 1.9689 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.06361\n",
      "Epoch 32/210\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 1.9580 - acc: 0.0967 - val_loss: 1.9681 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.06361\n",
      "Epoch 33/210\n",
      "42000/42000 [==============================] - 24s 576us/step - loss: 1.9576 - acc: 0.0967 - val_loss: 1.9673 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.06361\n",
      "Epoch 34/210\n",
      "42000/42000 [==============================] - 25s 593us/step - loss: 1.9571 - acc: 0.0967 - val_loss: 1.9665 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.06361\n",
      "Epoch 35/210\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 1.9566 - acc: 0.0967 - val_loss: 1.9657 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.06361\n",
      "Epoch 36/210\n",
      "42000/42000 [==============================] - 26s 612us/step - loss: 1.9562 - acc: 0.0967 - val_loss: 1.9650 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.06361\n",
      "Epoch 37/210\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 1.9557 - acc: 0.0967 - val_loss: 1.9642 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.06361\n",
      "Epoch 38/210\n",
      "42000/42000 [==============================] - 25s 593us/step - loss: 1.9552 - acc: 0.0967 - val_loss: 1.9634 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.06361\n",
      "Epoch 39/210\n",
      "42000/42000 [==============================] - 26s 614us/step - loss: 1.9548 - acc: 0.0967 - val_loss: 1.9627 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.06361\n",
      "Epoch 40/210\n",
      "42000/42000 [==============================] - 26s 612us/step - loss: 1.9543 - acc: 0.0967 - val_loss: 1.9619 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.06361\n",
      "Epoch 41/210\n",
      "42000/42000 [==============================] - 25s 605us/step - loss: 1.9539 - acc: 0.0967 - val_loss: 1.9612 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.06361\n",
      "Epoch 42/210\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 1.9535 - acc: 0.0967 - val_loss: 1.9604 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.06361\n",
      "Epoch 43/210\n",
      "42000/42000 [==============================] - 24s 575us/step - loss: 1.9530 - acc: 0.0967 - val_loss: 1.9597 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.06361\n",
      "Epoch 44/210\n",
      "42000/42000 [==============================] - 25s 605us/step - loss: 1.9526 - acc: 0.0967 - val_loss: 1.9589 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.06361\n",
      "Epoch 45/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9522 - acc: 0.0967 - val_loss: 1.9582 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.06361\n",
      "Epoch 46/210\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 1.9517 - acc: 0.0967 - val_loss: 1.9575 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.06361\n",
      "Epoch 47/210\n",
      "42000/42000 [==============================] - 24s 578us/step - loss: 1.9513 - acc: 0.0967 - val_loss: 1.9568 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.06361\n",
      "Epoch 48/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9509 - acc: 0.0967 - val_loss: 1.9561 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.06361\n",
      "Epoch 49/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9505 - acc: 0.0967 - val_loss: 1.9554 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.06361\n",
      "Epoch 50/210\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 1.9501 - acc: 0.0967 - val_loss: 1.9547 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.06361\n",
      "Epoch 51/210\n",
      "42000/42000 [==============================] - 26s 607us/step - loss: 1.9497 - acc: 0.0967 - val_loss: 1.9540 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.06361\n",
      "Epoch 52/210\n",
      "42000/42000 [==============================] - 25s 587us/step - loss: 1.9493 - acc: 0.0967 - val_loss: 1.9533 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.06361\n",
      "Epoch 53/210\n",
      "42000/42000 [==============================] - 26s 614us/step - loss: 1.9489 - acc: 0.0967 - val_loss: 1.9526 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.06361\n",
      "Epoch 54/210\n",
      "42000/42000 [==============================] - 28s 658us/step - loss: 1.9485 - acc: 0.0967 - val_loss: 1.9520 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.06361\n",
      "Epoch 55/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9481 - acc: 0.0967 - val_loss: 1.9513 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.06361\n",
      "Epoch 56/210\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 1.9477 - acc: 0.0967 - val_loss: 1.9506 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.06361\n",
      "Epoch 57/210\n",
      "42000/42000 [==============================] - 25s 596us/step - loss: 1.9473 - acc: 0.0967 - val_loss: 1.9500 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.06361\n",
      "Epoch 58/210\n",
      "42000/42000 [==============================] - 26s 622us/step - loss: 1.9469 - acc: 0.0967 - val_loss: 1.9493 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.06361\n",
      "Epoch 59/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9465 - acc: 0.0967 - val_loss: 1.9487 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.06361\n",
      "Epoch 60/210\n",
      "42000/42000 [==============================] - 25s 604us/step - loss: 1.9461 - acc: 0.0967 - val_loss: 1.9480 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.06361\n",
      "Epoch 61/210\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 1.9457 - acc: 0.0967 - val_loss: 1.9473 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.06361\n",
      "Epoch 62/210\n",
      "42000/42000 [==============================] - 25s 604us/step - loss: 1.9453 - acc: 0.0967 - val_loss: 1.9467 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.06361\n",
      "Epoch 63/210\n",
      "42000/42000 [==============================] - 26s 619us/step - loss: 1.9449 - acc: 0.0967 - val_loss: 1.9460 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.06361\n",
      "Epoch 64/210\n",
      "42000/42000 [==============================] - 26s 616us/step - loss: 1.9445 - acc: 0.0967 - val_loss: 1.9454 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.06361\n",
      "Epoch 65/210\n",
      "42000/42000 [==============================] - 26s 618us/step - loss: 1.9441 - acc: 0.0967 - val_loss: 1.9447 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.06361\n",
      "Epoch 66/210\n",
      "42000/42000 [==============================] - 26s 620us/step - loss: 1.9437 - acc: 0.0967 - val_loss: 1.9441 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.06361\n",
      "Epoch 67/210\n",
      "42000/42000 [==============================] - 27s 649us/step - loss: 1.9433 - acc: 0.0967 - val_loss: 1.9434 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.06361\n",
      "Epoch 68/210\n",
      "42000/42000 [==============================] - 26s 607us/step - loss: 1.9429 - acc: 0.0967 - val_loss: 1.9428 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.06361\n",
      "Epoch 69/210\n",
      "42000/42000 [==============================] - 27s 640us/step - loss: 1.9426 - acc: 0.0967 - val_loss: 1.9422 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.06361\n",
      "Epoch 70/210\n",
      "42000/42000 [==============================] - 27s 644us/step - loss: 1.9422 - acc: 0.0967 - val_loss: 1.9415 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.06361\n",
      "Epoch 71/210\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 1.9418 - acc: 0.0967 - val_loss: 1.9409 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.06361\n",
      "Epoch 72/210\n",
      "42000/42000 [==============================] - 27s 637us/step - loss: 1.9414 - acc: 0.0967 - val_loss: 1.9402 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.06361\n",
      "Epoch 73/210\n",
      "42000/42000 [==============================] - 26s 618us/step - loss: 1.9410 - acc: 0.0967 - val_loss: 1.9396 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.06361\n",
      "Epoch 74/210\n",
      "42000/42000 [==============================] - 27s 649us/step - loss: 1.9406 - acc: 0.0967 - val_loss: 1.9390 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.06361\n",
      "Epoch 75/210\n",
      "42000/42000 [==============================] - 26s 621us/step - loss: 1.9403 - acc: 0.0967 - val_loss: 1.9383 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.06361\n",
      "Epoch 76/210\n",
      "42000/42000 [==============================] - 24s 580us/step - loss: 1.9399 - acc: 0.0967 - val_loss: 1.9377 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.06361\n",
      "Epoch 77/210\n",
      "42000/42000 [==============================] - 25s 607us/step - loss: 1.9395 - acc: 0.0967 - val_loss: 1.9371 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.06361\n",
      "Epoch 78/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9391 - acc: 0.0967 - val_loss: 1.9365 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.06361\n",
      "Epoch 79/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9388 - acc: 0.0967 - val_loss: 1.9359 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.06361\n",
      "Epoch 80/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9384 - acc: 0.0967 - val_loss: 1.9353 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.06361\n",
      "Epoch 81/210\n",
      "42000/42000 [==============================] - 26s 608us/step - loss: 1.9381 - acc: 0.0967 - val_loss: 1.9347 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.06361\n",
      "Epoch 82/210\n",
      "42000/42000 [==============================] - 25s 606us/step - loss: 1.9377 - acc: 0.0967 - val_loss: 1.9341 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.06361\n",
      "Epoch 83/210\n",
      "42000/42000 [==============================] - 25s 584us/step - loss: 1.9374 - acc: 0.0967 - val_loss: 1.9335 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.06361\n",
      "Epoch 84/210\n",
      "42000/42000 [==============================] - 26s 614us/step - loss: 1.9370 - acc: 0.0967 - val_loss: 1.9329 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.06361\n",
      "Epoch 85/210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 25s 584us/step - loss: 1.9367 - acc: 0.0967 - val_loss: 1.9323 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.06361\n",
      "Epoch 86/210\n",
      "42000/42000 [==============================] - 25s 592us/step - loss: 1.9363 - acc: 0.0967 - val_loss: 1.9317 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.06361\n",
      "Epoch 87/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9360 - acc: 0.0967 - val_loss: 1.9311 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.06361\n",
      "Epoch 88/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9357 - acc: 0.0967 - val_loss: 1.9306 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.06361\n",
      "Epoch 89/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9353 - acc: 0.0967 - val_loss: 1.9300 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.06361\n",
      "Epoch 90/210\n",
      "42000/42000 [==============================] - 24s 574us/step - loss: 1.9350 - acc: 0.0967 - val_loss: 1.9295 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.06361\n",
      "Epoch 91/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9347 - acc: 0.0967 - val_loss: 1.9289 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.06361\n",
      "Epoch 92/210\n",
      "42000/42000 [==============================] - 24s 574us/step - loss: 1.9344 - acc: 0.0967 - val_loss: 1.9284 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.06361\n",
      "Epoch 93/210\n",
      "42000/42000 [==============================] - 25s 599us/step - loss: 1.9341 - acc: 0.0967 - val_loss: 1.9278 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.06361\n",
      "Epoch 94/210\n",
      "42000/42000 [==============================] - 24s 571us/step - loss: 1.9338 - acc: 0.0967 - val_loss: 1.9273 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.06361\n",
      "Epoch 95/210\n",
      "42000/42000 [==============================] - 25s 592us/step - loss: 1.9335 - acc: 0.0967 - val_loss: 1.9268 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.06361\n",
      "Epoch 96/210\n",
      "42000/42000 [==============================] - 28s 667us/step - loss: 1.9332 - acc: 0.0967 - val_loss: 1.9263 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.06361\n",
      "Epoch 97/210\n",
      "42000/42000 [==============================] - 24s 577us/step - loss: 1.9329 - acc: 0.0967 - val_loss: 1.9257 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.06361\n",
      "Epoch 98/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9326 - acc: 0.0967 - val_loss: 1.9252 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.06361\n",
      "Epoch 99/210\n",
      "42000/42000 [==============================] - 24s 579us/step - loss: 1.9323 - acc: 0.0967 - val_loss: 1.9247 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.06361\n",
      "Epoch 100/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9320 - acc: 0.0967 - val_loss: 1.9242 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.06361\n",
      "Epoch 101/210\n",
      "42000/42000 [==============================] - 26s 630us/step - loss: 1.9317 - acc: 0.0967 - val_loss: 1.9237 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.06361\n",
      "Epoch 102/210\n",
      "42000/42000 [==============================] - 25s 591us/step - loss: 1.9314 - acc: 0.0967 - val_loss: 1.9232 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.06361\n",
      "Epoch 103/210\n",
      "42000/42000 [==============================] - 26s 626us/step - loss: 1.9311 - acc: 0.0967 - val_loss: 1.9227 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.06361\n",
      "Epoch 104/210\n",
      "42000/42000 [==============================] - 24s 574us/step - loss: 1.9308 - acc: 0.0967 - val_loss: 1.9222 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.06361\n",
      "Epoch 105/210\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 1.9306 - acc: 0.0967 - val_loss: 1.9217 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.06361\n",
      "Epoch 106/210\n",
      "42000/42000 [==============================] - 24s 578us/step - loss: 1.9303 - acc: 0.0968 - val_loss: 1.9212 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.06361\n",
      "Epoch 107/210\n",
      "42000/42000 [==============================] - 25s 587us/step - loss: 1.9300 - acc: 0.0967 - val_loss: 1.9207 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.06361\n",
      "Epoch 108/210\n",
      "42000/42000 [==============================] - 26s 629us/step - loss: 1.9297 - acc: 0.0967 - val_loss: 1.9202 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.06361\n",
      "Epoch 109/210\n",
      "42000/42000 [==============================] - 24s 567us/step - loss: 1.9294 - acc: 0.0966 - val_loss: 1.9197 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.06361\n",
      "Epoch 110/210\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 1.9291 - acc: 0.0966 - val_loss: 1.9192 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.06361\n",
      "Epoch 111/210\n",
      "42000/42000 [==============================] - 25s 583us/step - loss: 1.9288 - acc: 0.0967 - val_loss: 1.9188 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.06361\n",
      "Epoch 112/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9286 - acc: 0.0966 - val_loss: 1.9183 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.06361\n",
      "Epoch 113/210\n",
      "42000/42000 [==============================] - 25s 602us/step - loss: 1.9283 - acc: 0.0968 - val_loss: 1.9178 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.06361\n",
      "Epoch 114/210\n",
      "42000/42000 [==============================] - 25s 597us/step - loss: 1.9280 - acc: 0.0968 - val_loss: 1.9173 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.06361\n",
      "Epoch 115/210\n",
      "42000/42000 [==============================] - 25s 605us/step - loss: 1.9277 - acc: 0.0969 - val_loss: 1.9168 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.06361\n",
      "Epoch 116/210\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 1.9274 - acc: 0.0970 - val_loss: 1.9164 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.06361\n",
      "Epoch 117/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9271 - acc: 0.0971 - val_loss: 1.9159 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.06361\n",
      "Epoch 118/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9269 - acc: 0.0973 - val_loss: 1.9154 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.06361\n",
      "Epoch 119/210\n",
      "42000/42000 [==============================] - 26s 607us/step - loss: 1.9266 - acc: 0.0975 - val_loss: 1.9149 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.06361\n",
      "Epoch 120/210\n",
      "42000/42000 [==============================] - 25s 607us/step - loss: 1.9263 - acc: 0.0975 - val_loss: 1.9144 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.06361\n",
      "Epoch 121/210\n",
      "42000/42000 [==============================] - 25s 602us/step - loss: 1.9260 - acc: 0.0980 - val_loss: 1.9140 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.06361\n",
      "Epoch 122/210\n",
      "42000/42000 [==============================] - 26s 616us/step - loss: 1.9257 - acc: 0.0989 - val_loss: 1.9135 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.06361\n",
      "Epoch 123/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9254 - acc: 0.0998 - val_loss: 1.9130 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.06361\n",
      "Epoch 124/210\n",
      "42000/42000 [==============================] - 27s 635us/step - loss: 1.9252 - acc: 0.1012 - val_loss: 1.9125 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.06361\n",
      "Epoch 125/210\n",
      "42000/42000 [==============================] - 28s 673us/step - loss: 1.9249 - acc: 0.1029 - val_loss: 1.9120 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.06361\n",
      "Epoch 126/210\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 1.9246 - acc: 0.1043 - val_loss: 1.9115 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.06361\n",
      "Epoch 127/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9243 - acc: 0.1060 - val_loss: 1.9110 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.06361\n",
      "Epoch 128/210\n",
      "42000/42000 [==============================] - 25s 584us/step - loss: 1.9240 - acc: 0.1067 - val_loss: 1.9105 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.06361\n",
      "Epoch 129/210\n",
      "42000/42000 [==============================] - 25s 591us/step - loss: 1.9238 - acc: 0.1080 - val_loss: 1.9101 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.06361\n",
      "Epoch 130/210\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 1.9235 - acc: 0.1099 - val_loss: 1.9096 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.06361\n",
      "Epoch 131/210\n",
      "42000/42000 [==============================] - 25s 596us/step - loss: 1.9232 - acc: 0.1129 - val_loss: 1.9091 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.06361\n",
      "Epoch 132/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9229 - acc: 0.1160 - val_loss: 1.9086 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.06361\n",
      "Epoch 133/210\n",
      "42000/42000 [==============================] - 24s 583us/step - loss: 1.9227 - acc: 0.1193 - val_loss: 1.9082 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.06361\n",
      "Epoch 134/210\n",
      "42000/42000 [==============================] - 26s 614us/step - loss: 1.9224 - acc: 0.1227 - val_loss: 1.9077 - val_acc: 0.0636\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.06361\n",
      "Epoch 135/210\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 1.9221 - acc: 0.1245 - val_loss: 1.9072 - val_acc: 0.0637\n",
      "\n",
      "Epoch 00135: val_acc improved from 0.06361 to 0.06367, saving model to best-weights.hdf5\n",
      "Epoch 136/210\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 1.9218 - acc: 0.1270 - val_loss: 1.9067 - val_acc: 0.0672\n",
      "\n",
      "Epoch 00136: val_acc improved from 0.06367 to 0.06717, saving model to best-weights.hdf5\n",
      "Epoch 137/210\n",
      "42000/42000 [==============================] - 25s 590us/step - loss: 1.9216 - acc: 0.1284 - val_loss: 1.9063 - val_acc: 0.0766\n",
      "\n",
      "Epoch 00137: val_acc improved from 0.06717 to 0.07656, saving model to best-weights.hdf5\n",
      "Epoch 138/210\n",
      "42000/42000 [==============================] - 25s 593us/step - loss: 1.9213 - acc: 0.1324 - val_loss: 1.9058 - val_acc: 0.0881\n",
      "\n",
      "Epoch 00138: val_acc improved from 0.07656 to 0.08806, saving model to best-weights.hdf5\n",
      "Epoch 139/210\n",
      "42000/42000 [==============================] - 27s 633us/step - loss: 1.9210 - acc: 0.1372 - val_loss: 1.9053 - val_acc: 0.1047\n",
      "\n",
      "Epoch 00139: val_acc improved from 0.08806 to 0.10472, saving model to best-weights.hdf5\n",
      "Epoch 140/210\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 1.9208 - acc: 0.1382 - val_loss: 1.9049 - val_acc: 0.1358\n",
      "\n",
      "Epoch 00140: val_acc improved from 0.10472 to 0.13578, saving model to best-weights.hdf5\n",
      "Epoch 141/210\n",
      "42000/42000 [==============================] - 26s 610us/step - loss: 1.9205 - acc: 0.1380 - val_loss: 1.9044 - val_acc: 0.1642\n",
      "\n",
      "Epoch 00141: val_acc improved from 0.13578 to 0.16422, saving model to best-weights.hdf5\n",
      "Epoch 142/210\n",
      "42000/42000 [==============================] - 25s 593us/step - loss: 1.9202 - acc: 0.1380 - val_loss: 1.9039 - val_acc: 0.1789\n",
      "\n",
      "Epoch 00142: val_acc improved from 0.16422 to 0.17894, saving model to best-weights.hdf5\n",
      "Epoch 143/210\n",
      "42000/42000 [==============================] - 25s 598us/step - loss: 1.9199 - acc: 0.1380 - val_loss: 1.9034 - val_acc: 0.1773\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.17894\n",
      "Epoch 144/210\n",
      "42000/42000 [==============================] - 25s 607us/step - loss: 1.9197 - acc: 0.1384 - val_loss: 1.9030 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.17894\n",
      "Epoch 145/210\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 1.9194 - acc: 0.1387 - val_loss: 1.9025 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.17894\n",
      "Epoch 146/210\n",
      "42000/42000 [==============================] - 25s 599us/step - loss: 1.9191 - acc: 0.1392 - val_loss: 1.9020 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.17894\n",
      "Epoch 147/210\n",
      "42000/42000 [==============================] - 25s 586us/step - loss: 1.9188 - acc: 0.1396 - val_loss: 1.9016 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.17894\n",
      "Epoch 148/210\n",
      "42000/42000 [==============================] - 25s 592us/step - loss: 1.9186 - acc: 0.1404 - val_loss: 1.9011 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.17894\n",
      "Epoch 149/210\n",
      "42000/42000 [==============================] - 25s 589us/step - loss: 1.9183 - acc: 0.1410 - val_loss: 1.9006 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.17894\n",
      "Epoch 150/210\n",
      "42000/42000 [==============================] - 25s 604us/step - loss: 1.9180 - acc: 0.1416 - val_loss: 1.9002 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.17894\n",
      "Epoch 151/210\n",
      "42000/42000 [==============================] - 26s 608us/step - loss: 1.9178 - acc: 0.1425 - val_loss: 1.8997 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.17894\n",
      "Epoch 152/210\n",
      "42000/42000 [==============================] - 26s 612us/step - loss: 1.9175 - acc: 0.1431 - val_loss: 1.8992 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.17894\n",
      "Epoch 153/210\n",
      "42000/42000 [==============================] - 26s 627us/step - loss: 1.9172 - acc: 0.1439 - val_loss: 1.8988 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.17894\n",
      "Epoch 154/210\n",
      "42000/42000 [==============================] - 26s 611us/step - loss: 1.9169 - acc: 0.1445 - val_loss: 1.8983 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.17894\n",
      "Epoch 155/210\n",
      "42000/42000 [==============================] - 25s 603us/step - loss: 1.9167 - acc: 0.1453 - val_loss: 1.8978 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.17894\n",
      "Epoch 156/210\n",
      "42000/42000 [==============================] - 26s 616us/step - loss: 1.9164 - acc: 0.1466 - val_loss: 1.8974 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.17894\n",
      "Epoch 157/210\n",
      "42000/42000 [==============================] - 25s 590us/step - loss: 1.9161 - acc: 0.1480 - val_loss: 1.8969 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.17894\n",
      "Epoch 158/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9159 - acc: 0.1504 - val_loss: 1.8964 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.17894\n",
      "Epoch 159/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9156 - acc: 0.1522 - val_loss: 1.8960 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.17894\n",
      "Epoch 160/210\n",
      "42000/42000 [==============================] - 26s 615us/step - loss: 1.9153 - acc: 0.1533 - val_loss: 1.8955 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.17894\n",
      "Epoch 161/210\n",
      "42000/42000 [==============================] - 26s 618us/step - loss: 1.9150 - acc: 0.1554 - val_loss: 1.8950 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.17894\n",
      "Epoch 162/210\n",
      "42000/42000 [==============================] - 25s 607us/step - loss: 1.9148 - acc: 0.1584 - val_loss: 1.8946 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.17894\n",
      "Epoch 163/210\n",
      "42000/42000 [==============================] - 27s 635us/step - loss: 1.9145 - acc: 0.1627 - val_loss: 1.8941 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.17894\n",
      "Epoch 164/210\n",
      "42000/42000 [==============================] - 26s 620us/step - loss: 1.9142 - acc: 0.1685 - val_loss: 1.8936 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.17894\n",
      "Epoch 165/210\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 1.9139 - acc: 0.1760 - val_loss: 1.8932 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.17894\n",
      "Epoch 166/210\n",
      "42000/42000 [==============================] - 26s 622us/step - loss: 1.9137 - acc: 0.1851 - val_loss: 1.8927 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.17894\n",
      "Epoch 167/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9134 - acc: 0.2015 - val_loss: 1.8922 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.17894\n",
      "Epoch 168/210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 25s 605us/step - loss: 1.9131 - acc: 0.2193 - val_loss: 1.8917 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.17894\n",
      "Epoch 169/210\n",
      "42000/42000 [==============================] - 24s 575us/step - loss: 1.9128 - acc: 0.2351 - val_loss: 1.8913 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.17894\n",
      "Epoch 170/210\n",
      "42000/42000 [==============================] - 26s 607us/step - loss: 1.9126 - acc: 0.2484 - val_loss: 1.8908 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.17894\n",
      "Epoch 171/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9123 - acc: 0.2630 - val_loss: 1.8903 - val_acc: 0.1771\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.17894\n",
      "Epoch 172/210\n",
      "42000/42000 [==============================] - 26s 621us/step - loss: 1.9120 - acc: 0.2706 - val_loss: 1.8899 - val_acc: 0.1797\n",
      "\n",
      "Epoch 00172: val_acc improved from 0.17894 to 0.17967, saving model to best-weights.hdf5\n",
      "Epoch 173/210\n",
      "42000/42000 [==============================] - 26s 631us/step - loss: 1.9118 - acc: 0.2810 - val_loss: 1.8894 - val_acc: 0.1882\n",
      "\n",
      "Epoch 00173: val_acc improved from 0.17967 to 0.18817, saving model to best-weights.hdf5\n",
      "Epoch 174/210\n",
      "42000/42000 [==============================] - 24s 580us/step - loss: 1.9115 - acc: 0.2945 - val_loss: 1.8889 - val_acc: 0.1992\n",
      "\n",
      "Epoch 00174: val_acc improved from 0.18817 to 0.19922, saving model to best-weights.hdf5\n",
      "Epoch 175/210\n",
      "42000/42000 [==============================] - 25s 604us/step - loss: 1.9112 - acc: 0.3090 - val_loss: 1.8884 - val_acc: 0.2082\n",
      "\n",
      "Epoch 00175: val_acc improved from 0.19922 to 0.20817, saving model to best-weights.hdf5\n",
      "Epoch 176/210\n",
      "42000/42000 [==============================] - 25s 590us/step - loss: 1.9110 - acc: 0.3174 - val_loss: 1.8880 - val_acc: 0.2182\n",
      "\n",
      "Epoch 00176: val_acc improved from 0.20817 to 0.21817, saving model to best-weights.hdf5\n",
      "Epoch 177/210\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 1.9107 - acc: 0.3240 - val_loss: 1.8875 - val_acc: 0.2336\n",
      "\n",
      "Epoch 00177: val_acc improved from 0.21817 to 0.23361, saving model to best-weights.hdf5\n",
      "Epoch 178/210\n",
      "42000/42000 [==============================] - 26s 629us/step - loss: 1.9104 - acc: 0.3289 - val_loss: 1.8870 - val_acc: 0.2447\n",
      "\n",
      "Epoch 00178: val_acc improved from 0.23361 to 0.24467, saving model to best-weights.hdf5\n",
      "Epoch 179/210\n",
      "42000/42000 [==============================] - 25s 601us/step - loss: 1.9101 - acc: 0.3330 - val_loss: 1.8866 - val_acc: 0.2602\n",
      "\n",
      "Epoch 00179: val_acc improved from 0.24467 to 0.26022, saving model to best-weights.hdf5\n",
      "Epoch 180/210\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 1.9099 - acc: 0.3352 - val_loss: 1.8861 - val_acc: 0.2804\n",
      "\n",
      "Epoch 00180: val_acc improved from 0.26022 to 0.28039, saving model to best-weights.hdf5\n",
      "Epoch 181/210\n",
      "42000/42000 [==============================] - 24s 578us/step - loss: 1.9096 - acc: 0.3361 - val_loss: 1.8857 - val_acc: 0.2901\n",
      "\n",
      "Epoch 00181: val_acc improved from 0.28039 to 0.29011, saving model to best-weights.hdf5\n",
      "Epoch 182/210\n",
      "42000/42000 [==============================] - 26s 608us/step - loss: 1.9093 - acc: 0.3366 - val_loss: 1.8852 - val_acc: 0.2947\n",
      "\n",
      "Epoch 00182: val_acc improved from 0.29011 to 0.29472, saving model to best-weights.hdf5\n",
      "Epoch 183/210\n",
      "42000/42000 [==============================] - 27s 640us/step - loss: 1.9091 - acc: 0.3362 - val_loss: 1.8847 - val_acc: 0.3047\n",
      "\n",
      "Epoch 00183: val_acc improved from 0.29472 to 0.30467, saving model to best-weights.hdf5\n",
      "Epoch 184/210\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 1.9088 - acc: 0.3362 - val_loss: 1.8843 - val_acc: 0.3145\n",
      "\n",
      "Epoch 00184: val_acc improved from 0.30467 to 0.31450, saving model to best-weights.hdf5\n",
      "Epoch 185/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9085 - acc: 0.3357 - val_loss: 1.8838 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00185: val_acc improved from 0.31450 to 0.33328, saving model to best-weights.hdf5\n",
      "Epoch 186/210\n",
      "42000/42000 [==============================] - 26s 612us/step - loss: 1.9083 - acc: 0.3343 - val_loss: 1.8834 - val_acc: 0.3432\n",
      "\n",
      "Epoch 00186: val_acc improved from 0.33328 to 0.34322, saving model to best-weights.hdf5\n",
      "Epoch 187/210\n",
      "42000/42000 [==============================] - 26s 629us/step - loss: 1.9080 - acc: 0.3345 - val_loss: 1.8829 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00187: val_acc improved from 0.34322 to 0.34378, saving model to best-weights.hdf5\n",
      "Epoch 188/210\n",
      "42000/42000 [==============================] - 26s 609us/step - loss: 1.9077 - acc: 0.3331 - val_loss: 1.8824 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.34378\n",
      "Epoch 189/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9075 - acc: 0.3333 - val_loss: 1.8820 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.34378\n",
      "Epoch 190/210\n",
      "42000/42000 [==============================] - 26s 611us/step - loss: 1.9072 - acc: 0.3341 - val_loss: 1.8815 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.34378\n",
      "Epoch 191/210\n",
      "42000/42000 [==============================] - 25s 586us/step - loss: 1.9070 - acc: 0.3351 - val_loss: 1.8811 - val_acc: 0.3437\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.34378\n",
      "Epoch 192/210\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 1.9067 - acc: 0.3325 - val_loss: 1.8806 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00192: val_acc improved from 0.34378 to 0.34378, saving model to best-weights.hdf5\n",
      "Epoch 193/210\n",
      "42000/42000 [==============================] - 25s 588us/step - loss: 1.9064 - acc: 0.3317 - val_loss: 1.8802 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00193: val_acc improved from 0.34378 to 0.34383, saving model to best-weights.hdf5\n",
      "Epoch 194/210\n",
      "42000/42000 [==============================] - 25s 595us/step - loss: 1.9062 - acc: 0.3317 - val_loss: 1.8797 - val_acc: 0.3446\n",
      "\n",
      "Epoch 00194: val_acc improved from 0.34383 to 0.34461, saving model to best-weights.hdf5\n",
      "Epoch 195/210\n",
      "42000/42000 [==============================] - 26s 618us/step - loss: 1.9059 - acc: 0.3316 - val_loss: 1.8792 - val_acc: 0.3457\n",
      "\n",
      "Epoch 00195: val_acc improved from 0.34461 to 0.34567, saving model to best-weights.hdf5\n",
      "Epoch 196/210\n",
      "42000/42000 [==============================] - 25s 607us/step - loss: 1.9056 - acc: 0.3313 - val_loss: 1.8788 - val_acc: 0.3463\n",
      "\n",
      "Epoch 00196: val_acc improved from 0.34567 to 0.34633, saving model to best-weights.hdf5\n",
      "Epoch 197/210\n",
      "42000/42000 [==============================] - 28s 669us/step - loss: 1.9053 - acc: 0.3313 - val_loss: 1.8783 - val_acc: 0.3462\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.34633\n",
      "Epoch 198/210\n",
      "42000/42000 [==============================] - 26s 626us/step - loss: 1.9051 - acc: 0.3314 - val_loss: 1.8779 - val_acc: 0.3449\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.34633\n",
      "Epoch 199/210\n",
      "42000/42000 [==============================] - 26s 615us/step - loss: 1.9048 - acc: 0.3316 - val_loss: 1.8774 - val_acc: 0.3440\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.34633\n",
      "Epoch 200/210\n",
      "42000/42000 [==============================] - 27s 648us/step - loss: 1.9045 - acc: 0.3312 - val_loss: 1.8769 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.34633\n",
      "Epoch 201/210\n",
      "42000/42000 [==============================] - 27s 638us/step - loss: 1.9043 - acc: 0.3313 - val_loss: 1.8765 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.34633\n",
      "Epoch 202/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9040 - acc: 0.3308 - val_loss: 1.8760 - val_acc: 0.3441\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.34633\n",
      "Epoch 203/210\n",
      "42000/42000 [==============================] - 26s 623us/step - loss: 1.9037 - acc: 0.3302 - val_loss: 1.8756 - val_acc: 0.3429\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.34633\n",
      "Epoch 204/210\n",
      "42000/42000 [==============================] - 27s 633us/step - loss: 1.9035 - acc: 0.3300 - val_loss: 1.8751 - val_acc: 0.3439\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.34633\n",
      "Epoch 205/210\n",
      "42000/42000 [==============================] - 25s 594us/step - loss: 1.9032 - acc: 0.3304 - val_loss: 1.8747 - val_acc: 0.3442\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.34633\n",
      "Epoch 206/210\n",
      "42000/42000 [==============================] - 25s 600us/step - loss: 1.9029 - acc: 0.3313 - val_loss: 1.8742 - val_acc: 0.3462\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.34633\n",
      "Epoch 207/210\n",
      "42000/42000 [==============================] - 24s 582us/step - loss: 1.9027 - acc: 0.3310 - val_loss: 1.8737 - val_acc: 0.3456\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.34633\n",
      "Epoch 208/210\n",
      "42000/42000 [==============================] - 26s 613us/step - loss: 1.9024 - acc: 0.3306 - val_loss: 1.8733 - val_acc: 0.3441\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.34633\n",
      "Epoch 209/210\n",
      "42000/42000 [==============================] - 26s 617us/step - loss: 1.9021 - acc: 0.3308 - val_loss: 1.8728 - val_acc: 0.3421\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.34633\n",
      "Epoch 210/210\n",
      "42000/42000 [==============================] - 25s 602us/step - loss: 1.9018 - acc: 0.3309 - val_loss: 1.8723 - val_acc: 0.3417\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.34633\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='best-weights.hdf5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "model = Sequential()\n",
    "# model.add(layers.CuDNNGRU(\n",
    "#     units=250, \n",
    "#     input_shape=(None, f_data_train.shape[-1]),\n",
    "#     return_sequences=False,\n",
    "#     kernel_regularizer=None,\n",
    "#     recurrent_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None))\n",
    "# model.add(layers.Activation('tanh'))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=lookback, activation='relu', input_dim=f_data_train.shape[-1]))\n",
    "model.add(layers.Dense(100, activation = 'relu'))\n",
    "model.add(layers.Dense(100, activation = 'relu'))\n",
    "model.add(layers.Dense(100, activation = 'relu'))\n",
    "model.add(layers.Dense(units=7, activation='softmax'))\n",
    "sgd = optimizers.Adam(lr=0.00000001)\n",
    "#sgd = optimizers.SGD(lr=0.00001, momentum=0.0, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#history = model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=10, validation_data=val_gen, validation_steps=val_steps, callbacks=[checkpoint])\n",
    "print('fitting')\n",
    "history = model.fit(f_data_train, f_data_train_y, epochs=210, validation_split=0.3, callbacks=[checkpoint], verbose=1, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               3100      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 24,937\n",
      "Trainable params: 24,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('weights.last.hdf5')\n",
    "model.load_weights('best-weights.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24937577, 0.24937577, 0.24907684, 0.24912959, 0.24932302,\n",
       "       0.24960436, 0.24962194, 0.24955161, 0.24956919, 0.24960436,\n",
       "       0.24919993, 0.2496747 , 0.24944611, 0.24962194, 0.2496747 ,\n",
       "       0.24946369, 0.24941094, 0.24941094, 0.24930543, 0.24921751,\n",
       "       0.24939335, 0.24919993, 0.24900651, 0.248901  , 0.24888342,\n",
       "       0.24898892, 0.24888342, 0.24876033, 0.24895375, 0.24895375,\n",
       "       0.24895375, 0.24898892, 0.24891859, 0.24897134, 0.24897134,\n",
       "       0.24891859, 0.24895375, 0.24886583, 0.24877791, 0.24919993,\n",
       "       0.24928785, 0.2492351 , 0.24944611, 0.24956919, 0.24916476,\n",
       "       0.24916476, 0.24883067, 0.24881308, 0.24900651, 0.24874275,\n",
       "       0.24877791, 0.24858449, 0.24884825, 0.24888342, 0.24911201,\n",
       "       0.248901  , 0.24881308, 0.24881308, 0.24842624, 0.24842624,\n",
       "       0.2487955 , 0.24876033, 0.24860207, 0.24823281, 0.24795147,\n",
       "       0.24777563, 0.24779321, 0.24767012, 0.24767012, 0.24751187,\n",
       "       0.24772288, 0.24754704, 0.24754704, 0.2474767 , 0.24754704,\n",
       "       0.24763496, 0.24763496, 0.24735361, 0.24754704, 0.24765254,\n",
       "       0.24788113, 0.24793388, 0.2480218 , 0.24798664, 0.24784596,\n",
       "       0.24784596, 0.24800422, 0.24784596, 0.24774046, 0.24775804])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(f_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 3\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 5\n",
      "1 5\n",
      "1 0\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 6\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 0\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 5\n",
      "1 3\n",
      "1 0\n",
      "1 4\n",
      "1 6\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 0\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 0\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 6\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 0\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 1\n",
      "1 6\n",
      "1 4\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 2\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 0\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 3\n",
      "1 2\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 2\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 0\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 6\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 3\n",
      "1 0\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 0\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 0\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 0\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 0\n",
      "1 0\n",
      "1 5\n",
      "1 5\n",
      "1 6\n",
      "1 6\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 3\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 2\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 1\n",
      "1 0\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 4\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 6\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 1\n",
      "1 6\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 2\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 2\n",
      "1 5\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 0\n",
      "1 5\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 4\n",
      "1 2\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 2\n",
      "1 3\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 2\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 0\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 5\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 5\n",
      "1 4\n",
      "1 4\n",
      "1 6\n",
      "1 0\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 2\n",
      "1 2\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 1\n",
      "1 5\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 1\n",
      "1 4\n",
      "1 5\n",
      "1 4\n",
      "1 1\n",
      "1 3\n",
      "1 4\n",
      "1 1\n",
      "1 5\n",
      "1 0\n",
      "1 4\n",
      "1 5\n",
      "1 1\n",
      "1 1\n",
      "1 4\n",
      "1 2\n",
      "1 1\n",
      "1 3\n",
      "1 5\n",
      "1 2\n",
      "1 4\n",
      "1 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(np.argmax(scores[i]), np.argmax(f_data_test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00191043])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = scores[0] - test_y[0]\n",
    "scores_dx = scores - dx\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1022304]\n",
      "[1.10032]\n"
     ]
    }
   ],
   "source": [
    "print(scores[0])\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### from matplotlib import pyplot as plt\n",
    "plt.plot(range(len(scores)), scores_dx, 'r', label='Predicted')\n",
    "plt.plot(range(test_steps), test_y, 'b', label='Real')\n",
    "plt.legend(['Predicted', 'Real'])\n",
    "plt.title('EURUSD Real and predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(len(scores)), scores, 'r', label='Predicted')\n",
    "#plt.legend(['Predicted', 'Real'])\n",
    "plt.title('EURUSD Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights.last.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
